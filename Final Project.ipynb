{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jared Balkman\n",
    "#DS710\n",
    "#Final Project - Python - Data Gathering and Processing\n",
    "#16 December, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "#for the Twitter session and collecting tweet data\n",
    "import tweepy\n",
    "\n",
    "#for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#to save the raw data before processing\n",
    "import pickle\n",
    "\n",
    "#for polarity and subjectivity scores, respectively\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "#for removing URLs and username mentions in the tweet text\n",
    "#I added comments where I used regular expressions (two lines of code) to show my understanding of what they're doing\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the script containing my Twitter credentials\n",
    "%run C:/Users/jared/OneDrive/Desktop/DS710/twitter_credentials.py\n",
    "    \n",
    "#authenticate\n",
    "auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)\n",
    "auth.set_access_token(acc_token, acc_secret)\n",
    "\n",
    "#Connect to the Twitter API using the authentication\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we'll define all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: For some reason I couldn't pass the date in as an argument and get a successful search, so I kindly ask that \n",
    "#for your purposes (assuming 2020-12-14 is more than 7 days ago) you adjust the date below in the api.search function\n",
    "\n",
    "def get_x_tweets(num_needed):\n",
    "    \"\"\"Take in the number of tweets needed, perform a REST API search using the 10 most common words on Twitter,\n",
    "       and return the search results in a list\"\"\"\n",
    "    \n",
    "    tweet_list = []\n",
    "    last_id = -1 # id of last tweet seen\n",
    "    while len(tweet_list) < num_needed:\n",
    "        \n",
    "        try:\n",
    "            new_tweets = api.search(q = 'the OR i OR to OR a OR and OR is OR in OR it OR you OR of until:2020-12-14',\n",
    "                                    \n",
    "                                    #restrict to English language because sentiment dictionaries are only in English\n",
    "                                    lang='en',\n",
    "                                    \n",
    "                                    #restrict the count to the rate limit per 15 minute window\n",
    "                                    count = 180,\n",
    "                                    max_id = str(last_id - 1),\n",
    "                                    \n",
    "                                    #extended tweet mode to make sure we get the full text of each tweet\n",
    "                                    tweet_mode='extended',\n",
    "                                    wait_on_rate_limit=True,\n",
    "                                    wait_on_rate_limit_notify=True)\n",
    "            \n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Error\", e)\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            if not new_tweets:\n",
    "                print(\"Could not find any more tweets!\")\n",
    "                break\n",
    "            tweet_list.extend(new_tweets)\n",
    "            last_id = new_tweets[-1].id\n",
    "            \n",
    "    return tweet_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mostly for archival purposes but also in case I goofed something up and had to start over\n",
    "\n",
    "def pickle_raw_data(filename, data):\n",
    "    \"\"\"Takes a data object and pickles it with the given filename\"\"\"\n",
    "    \n",
    "    with open('tweet_list.pickle', 'wb') as raw_tweets:\n",
    "        pickle.dump(tweet_list, raw_tweets, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_tweet_json(tweet_list):\n",
    "    \"\"\"Gets the _json dictionaries each Tweet Status object in a list and puts them all in a pandas dataframe\"\"\"\n",
    "    \n",
    "    tweet_list_json = [tweet_list[x]._json for x in range(len(tweet_list))]\n",
    "    df = pd.DataFrame(tweet_list_json)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_retweets(df):\n",
    "    \"\"\"Adds a logical column to the dataframe with value True if the tweet is a retweet and False if an original tweet\"\"\"\n",
    "    \n",
    "    #pre-allocate the column\n",
    "    df.loc[:, 'is_retweet'] = 0\n",
    "    \n",
    "    #change 'retweeted_status' attribute of original tweets from NaN to 0\n",
    "    df['retweeted_status'] = df['retweeted_status'].fillna(0)\n",
    "    \n",
    "    #fill in the values for the 'is_retweet' column\n",
    "    df.loc[df['retweeted_status'] != 0, 'is_retweet'] = True\n",
    "    df.loc[df['retweeted_status'] == 0, 'is_retweet'] = False\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_for_analysis(df, list_of_attributes):\n",
    "    \"\"\"This takes a list of attributes from the _json dataframe and creates a new dataframe of those attributes. Some of\n",
    "       these attributes - namely 'user' and 'retweeted_status' are dictionaries themselves, and we'll use this function\n",
    "       again on those dictionaries to get the attributes we want out of there\"\"\"\n",
    "    \n",
    "    df_subset = df[list_of_attributes]\n",
    "    \n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dfs(frames):\n",
    "    \"\"\"Concatenates a list of dataframes horizontally along the x-axis\"\"\"\n",
    "    \n",
    "    df_flattened = pd.concat(frames, axis=1)\n",
    "    \n",
    "    return df_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(df, list_of_column_lists):\n",
    "    \"\"\"For a given dataframe, combines pairs of columns from that dataframe where the first column's NaN values are \n",
    "       replace with the second column's values for corresponding rows. Then, deletes the second column and returns\n",
    "       the modified dataframe\"\"\"\n",
    "    \n",
    "    for column_pair in list_of_column_lists:\n",
    "        column_pair[0].fillna(column_pair[1], inplace=True)\n",
    "        del column_pair[1]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_text(df, text_column):\n",
    "    \"\"\"Makes a list of strings from a dataframe column, removes URLs, user mentions, and newline characters from each\n",
    "       string, and replaces the old dataframe column with a cleaned version. Returns the updated dataframe\"\"\"\n",
    "    \n",
    "    tweettext = [text_column[i] for i in range(len(text_column))]\n",
    "    \n",
    "    #Here are my regular expression substitutions with comments for what's going on:\n",
    "    \n",
    "    #Remove URLs:\n",
    "    #'r' - Python will not interpret backslash sequences. Needed because of the backslashes in URLs\n",
    "    #'http' - the beginning of the string to search for\n",
    "    #'\\' - indicates a special sequence\n",
    "    #'S' - that sequence being all following non-whitespace characters. In other words, the entirety of the URL\n",
    "    #'+' - matches repititions in case there are multiple URLs\n",
    "    \n",
    "    tweettext = [re.sub(r'http\\S+', '', tweettext[i]) for i in range(len(tweettext))]\n",
    "    \n",
    "    #same as above, except we don't worry about backslashes in the @username sequence\n",
    "    tweettext = [re.sub('@\\S+', '', tweettext[i]) for i in range(len(tweettext))]\n",
    "    \n",
    "    #Finally, remove '\\n' characters\n",
    "    tweettext_clean = [tweettext[i].replace('\\n', '') for i in range(len(tweettext))]\n",
    "    \n",
    "    #And replace the text column with the cleaned version:\n",
    "    df['text_column'] = tweettext_clean\n",
    "    \n",
    "    return df\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_and_subj_scores(df, list_of_strings):\n",
    "    \"\"\"Computes TextBlob subjectivity score and vaderSentiment compound polarity score for each string in a list;\n",
    "       adds those scores as attributes in a dataframe and returns the updated dataframe\"\"\"\n",
    "    #TextBlob polarity, a score from -1 (negative) to 1 (positive). Ultimately not used for analysis\n",
    "    df['textblob_polarity'] = [TextBlob(list_of_strings[i]).polarity for i in range(len(df))]\n",
    "    #TextBlob subjectivity, a score from 0 (objective) to 1 (most subjective)\n",
    "    df['textblob_subjectivity'] = [TextBlob(list_of_strings[i]).subjectivity for i in range(len(df))]\n",
    "\n",
    "    #vaderSentiment polarity. vaderSentiment calculates a positive, negative, and neutral score for a string, and then\n",
    "    #averages them all into one 'compound' score that will be used for this analysis. But all four values were collected\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = [analyzer.polarity_scores(sentence) for sentence in list_of_strings]\n",
    "    \n",
    "    #add the scores to the df and then only keep the compound score. Probably a more efficient way to do this\n",
    "    df['vaderSentiment_polarity_scores'] = vs\n",
    "    \n",
    "    df = pd.concat([df.drop(['vaderSentiment_polarity_scores'], axis=1),\n",
    "                         pd.DataFrame(df['vaderSentiment_polarity_scores'].tolist())], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, attribute):\n",
    "    \"\"\"Removes rows of a dataframe based on duplicate values of an attribute in that dataframe,\n",
    "       and returns the dataframe.\"\"\"\n",
    "    \n",
    "    df.drop_duplicates([attribute], inplace=True, ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rate_limit_context': {'access_token': '1075678699-KyvUfsgguposFgmoluF28hksrqyGNxTdFwmCO4N'},\n",
       " 'resources': {'search': {'/search/tweets': {'limit': 180,\n",
       "    'remaining': 180,\n",
       "    'reset': 1608186805}}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Somehow even with the wait_on_rate_limit arguments passed when connecting to the API,\n",
    "#I was still getting rate limit errors for a bit. I used this line to check when I could\n",
    "#do another search\n",
    "\n",
    "api.rate_limit_status('search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the tweets. I got back 10037 using num_needed = 10000. The number of tweets obviously can be adjusted to preference\n",
    "tweet_list = get_x_tweets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the _json and create a pandas dataframe\n",
    "df = pd.DataFrame(df_from_tweet_json(tweet_list))\n",
    "\n",
    "#Make the boolean column 'is_retweet'\n",
    "df = classify_retweets(df)\n",
    "\n",
    "#Get the subset of the dataframe we're interested in\n",
    "df_tweets = subset_for_analysis(df, ['created_at',\n",
    "                                     'id',\n",
    "                                     'full_text',\n",
    "                                     'user',\n",
    "                                     'retweeted_status',\n",
    "                                     'retweet_count',\n",
    "                                     'favorite_count',\n",
    "                                     'is_retweet'])\n",
    "\n",
    "#Now we need to make new dataframes from the 'user' and 'retweeted_status' attributes, because they contain the user\n",
    "#info and, for retweets, info on the original tweet that we want for analysis\n",
    "df_tweets_userinfo = df_tweets['user'].apply(pd.Series)\n",
    "df_tweets_retweetinfo = df_tweets['retweeted_status'].apply(pd.Series)\n",
    "\n",
    "#Next, get the subsets of those dataframes that we're interested in\n",
    "df_tweets_userinfo = subset_for_analysis(df_tweets_userinfo, ['id',\n",
    "                                                              'name',\n",
    "                                                              'screen_name',\n",
    "                                                              'followers_count',\n",
    "                                                              'verified'])\n",
    "\n",
    "df_tweets_retweetinfo = subset_for_analysis(df_tweets_retweetinfo, ['created_at',\n",
    "                                                                    'id',\n",
    "                                                                    'full_text',\n",
    "                                                                    'user',\n",
    "                                                                    'retweet_count',\n",
    "                                                                    'favorite_count'])\n",
    "\n",
    "#We have to do this one more time, for the 'user' attribute in df_tweets_retweetinfo, to get the user info for the\n",
    "#author of the original tweet\n",
    "df_tweets_retweetinfo_userinfo = df_tweets_retweetinfo['user'].apply(pd.Series)\n",
    "df_tweets_retweetinfo_userinfo = subset_for_analysis(df_tweets_retweetinfo_userinfo, ['id',\n",
    "                                                                                      'name',\n",
    "                                                                                      'screen_name',\n",
    "                                                                                      'followers_count',\n",
    "                                                                                      'verified'])\n",
    "\n",
    "#Now we have all the attributes we want in four dataframes. Time to concatenate:\n",
    "frames = [df_tweets, df_tweets_userinfo, df_tweets_retweetinfo, df_tweets_retweetinfo_userinfo]\n",
    "df_flattened = concat_dfs(frames)\n",
    "\n",
    "#The next line drops the original 'user' and 'retweeted_status' attributes from the dataframe since we have what we\n",
    "#want out of there. This is not the best way to do this.\n",
    "df_flattened.drop(df_flattened.iloc[:,[3,4]], axis=1, inplace=True)\n",
    "\n",
    "#Now I need to rename some attributes so they're all unique:\n",
    "df_flattened.columns = ['created_at',\n",
    "                        'id', \n",
    "                        'full_text',\n",
    "                        'retweet_count',\n",
    "                        'favorite_count',\n",
    "                        'is_retweet',\n",
    "                        'user_id',\n",
    "                        'user_name',\n",
    "                        'user_screen_name',\n",
    "                        'followers_count',\n",
    "                        'verified',\n",
    "                        'orig_created_at',\n",
    "                        'orig_id',\n",
    "                        'orig_full_text',\n",
    "                        'orig_retweet_count',\n",
    "                        'orig_favorite_count',\n",
    "                        'orig_user_id',\n",
    "                        'orig_user_name',\n",
    "                        'orig_screen_name',\n",
    "                        'orig_followers_count',\n",
    "                        'orig_verified']\n",
    "\n",
    "#All of the 'orig' columns pertain to retweet data, so these have NaN values in the case of original tweets. We'll\n",
    "#replace the NaN values with the values from their corresponding columns, to ultimately have one list of all, and only,\n",
    "#original tweet data\n",
    "\n",
    "df_flattened = combine_columns(df_flattened, [[df_flattened.orig_full_text, df_flattened.full_text],\n",
    "                                              [df_flattened.orig_created_at, df_flattened.created_at],\n",
    "                                              [df_flattened.orig_id, df_flattened.id],\n",
    "                                              [df_flattened.orig_retweet_count, df_flattened.retweet_count],\n",
    "                                              [df_flattened.orig_favorite_count, df_flattened.favorite_count],\n",
    "                                              [df_flattened.orig_user_id, df_flattened.user_id],\n",
    "                                              [df_flattened.orig_user_name, df_flattened.user_name],\n",
    "                                              [df_flattened.orig_screen_name, df_flattened.user_screen_name],\n",
    "                                              [df_flattened.orig_followers_count, df_flattened.followers_count],\n",
    "                                              [df_flattened.orig_verified, df_flattened.verified]])\n",
    "\n",
    "#Next, clean the tweet text:\n",
    "df_flattened = clean_tweet_text(df_flattened, list(df_flattened['orig_full_text']))\n",
    "\n",
    "\n",
    "\n",
    "#Assign polarity and subjectivity scores\n",
    "df_flattened = pol_and_subj_scores(df_flattened, list(df_flattened.orig_full_text))\n",
    "\n",
    "#Remove duplicates based on original tweet id (My results gave 888 duplicates removed to give remaining dataset of \n",
    "#9419 rows x 17 columns)\n",
    "df_flattened = remove_duplicates(df_flattened, 'orig_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_retweets</th>\n",
       "      <th>num_likes</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90665</td>\n",
       "      <td>542299</td>\n",
       "      <td>88626936</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>868</td>\n",
       "      <td>1682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>392</td>\n",
       "      <td>2307</td>\n",
       "      <td>966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>569</td>\n",
       "      <td>3462</td>\n",
       "      <td>23954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24415</td>\n",
       "      <td>131209</td>\n",
       "      <td>6733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>455</td>\n",
       "      <td>542</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>57253</td>\n",
       "      <td>195563</td>\n",
       "      <td>16068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>46</td>\n",
       "      <td>401</td>\n",
       "      <td>25791362</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.7579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1037 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_retweets  num_likes  num_followers  subjectivity  polarity\n",
       "0            90665     542299       88626936      0.750000    0.0000\n",
       "1              128        868           1682      0.000000    0.3400\n",
       "2              392       2307            966      0.000000    0.0000\n",
       "3              569       3462          23954      0.000000    0.6476\n",
       "4            24415     131209           6733      0.000000    0.0000\n",
       "...            ...        ...            ...           ...       ...\n",
       "1032           455        542              9      0.000000    0.7125\n",
       "1033         57253     195563          16068      0.000000   -0.5423\n",
       "1034             0          1              8      0.300000   -0.2960\n",
       "1035            46        401       25791362      0.633333    0.7579\n",
       "1036             0          2            440      0.000000    0.4404\n",
       "\n",
       "[1037 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the whole dataframe as a CSV (n=9149 for me)\n",
    "df_flattened.to_csv('twitter_data.csv')\n",
    "#Subset the first 100 rows for submission\n",
    "df_flattened.iloc[:100, :].to_csv('twitter_data_100_tweets.csv')\n",
    "\n",
    "#Subset the data to be used for R\n",
    "twitter_data_for_r = subset_for_analysis(df_flattened, ['orig_retweet_count',\n",
    "                                                        'orig_favorite_count',\n",
    "                                                        'orig_followers_count',\n",
    "                                                        'textblob_subjectivity',\n",
    "                                                        'compound'])\n",
    "\n",
    "#Rename the columns appropriately\n",
    "twitter_data_for_r.columns = ['num_retweets',\n",
    "                              'num_likes',\n",
    "                              'num_followers',\n",
    "                              'subjectivity',\n",
    "                              'polarity']\n",
    "\n",
    "#Originally I let Python figure out the dtypes and convert them, but they all showed up as doubles in R anyway\n",
    "twitter_data_for_r = twitter_data_for_r.convert_dtypes()\n",
    "twitter_data_for_r.dtypes\n",
    "\n",
    "#Write to CSV\n",
    "twitter_data_for_r.to_csv('twitter_data_for_r.csv')\n",
    "\n",
    "#Subset the first 1000 rows for submission\n",
    "twitter_data_for_r.iloc[:1000, :].to_csv('twitter_data_for_r_1000_rows.csv')\n",
    "\n",
    "twitter_data_for_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_retweets</th>\n",
       "      <th>num_likes</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_retweet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>596</td>\n",
       "      <td>596</td>\n",
       "      <td>596</td>\n",
       "      <td>596</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>441</td>\n",
       "      <td>441</td>\n",
       "      <td>441</td>\n",
       "      <td>441</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            num_retweets  num_likes  num_followers  subjectivity  polarity\n",
       "is_retweet                                                                \n",
       "False                596        596            596           596       596\n",
       "True                 441        441            441           441       441"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of retweets in the dataset (mine was roughly 40%)\n",
    "twitter_data_for_r.groupby(df_flattened.is_retweet).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9772 0.9891\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "#Min/max scores for polarity and subjectivity\n",
    "print(twitter_data_for_r.polarity.min(), twitter_data_for_r.polarity.max())\n",
    "print(twitter_data_for_r.subjectivity.min(), twitter_data_for_r.subjectivity.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
